{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cc6ee422",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\12har\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# First, import all the libraries.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9cda75b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your dataset\n",
    "df = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "# Data Preprocessing\n",
    "# Fill missing values\n",
    "df['location'].fillna('No Location', inplace=True)\n",
    "df['keyword'].fillna('No Keyword', inplace=True)\n",
    "\n",
    "# Text cleaning function\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'http\\S+', ' ', text)  # remove urls\n",
    "    text = re.sub(r'<.*?>', ' ', text)    # remove html tags\n",
    "    text = re.sub(r'\\d+', ' ', text)      # remove digits\n",
    "    text = re.sub(r'#\\w+', ' ', text)     # remove hashtags\n",
    "    text = re.sub(r'@\\w+', ' ', text)     # remove mentions\n",
    "    text = re.sub(r'\\s+', ' ', text)      # remove extra whitespace\n",
    "    text = text.lower()                   # convert to lowercase\n",
    "    return text\n",
    "\n",
    "# Apply text cleaning function\n",
    "df['text'] = df['text'].apply(clean_text)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['target'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74b83779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7925147734734077\n",
      "Confusion Matrix:\n",
      " [[771 103]\n",
      " [213 436]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.88      0.83       874\n",
      "           1       0.81      0.67      0.73       649\n",
      "\n",
      "    accuracy                           0.79      1523\n",
      "   macro avg       0.80      0.78      0.78      1523\n",
      "weighted avg       0.79      0.79      0.79      1523\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Simple Logistic Regression model with TF-IDF for text vectorization\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(stop_words=stopwords.words('english'))),\n",
    "    ('clf', LogisticRegression(solver='liblinear')),\n",
    "])\n",
    "\n",
    "# Train the model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Model Evaluation\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "39bc0686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7734734077478661\n"
     ]
    }
   ],
   "source": [
    "# 1-2. logistic regression model with feature engineering\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# One-Hot Encoding for 'keyword' and 'location'\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "encoded_keywords = encoder.fit_transform(df[['keyword']])\n",
    "encoded_locations = encoder.fit_transform(df[['location']])\n",
    "\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "tfidf = TfidfVectorizer(stop_words=stopwords.words('english'))\n",
    "tfidf_text = tfidf.fit_transform(df['text'])\n",
    "\n",
    "# Combine TF-IDF with encoded keyword and location\n",
    "combined_features = hstack([tfidf_text, encoded_keywords, encoded_locations])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(combined_features, df['target'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Logistic Regression\n",
    "clf = LogisticRegression(solver='liblinear')\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b11e2170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7918581746552856\n",
      "Confusion Matrix:\n",
      " [[782  92]\n",
      " [225 424]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.89      0.83       874\n",
      "           1       0.82      0.65      0.73       649\n",
      "\n",
      "    accuracy                           0.79      1523\n",
      "   macro avg       0.80      0.77      0.78      1523\n",
      "weighted avg       0.80      0.79      0.79      1523\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2. Naive Bayes\n",
    "\n",
    "pipeline_nb = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(stop_words=stopwords.words('english'))),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "\n",
    "pipeline_nb.fit(X_train, y_train)\n",
    "y_pred_nb = pipeline_nb.predict(X_test)\n",
    "\n",
    "# Model Evaluation\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_nb))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_nb))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_nb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3d50cfcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7879185817465528\n",
      "Confusion Matrix:\n",
      " [[743 131]\n",
      " [192 457]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.85      0.82       874\n",
      "           1       0.78      0.70      0.74       649\n",
      "\n",
      "    accuracy                           0.79      1523\n",
      "   macro avg       0.79      0.78      0.78      1523\n",
      "weighted avg       0.79      0.79      0.79      1523\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. Support Vector Machine\n",
    "\n",
    "pipeline_svc = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(stop_words=stopwords.words('english'))),\n",
    "    ('clf', SVC(kernel='linear')),\n",
    "])\n",
    "\n",
    "pipeline_svc.fit(X_train, y_train)\n",
    "y_pred_svc = pipeline_svc.predict(X_test)\n",
    "\n",
    "# Model Evaluation\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_svc))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_svc))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_svc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c4a606ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7603414313854235\n",
      "Confusion Matrix:\n",
      " [[685 189]\n",
      " [176 473]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.78      0.79       874\n",
      "           1       0.71      0.73      0.72       649\n",
      "\n",
      "    accuracy                           0.76      1523\n",
      "   macro avg       0.76      0.76      0.76      1523\n",
      "weighted avg       0.76      0.76      0.76      1523\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4. Random Forest\n",
    "\n",
    "pipeline_rf = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(stop_words=stopwords.words('english'))),\n",
    "    ('clf', RandomForestClassifier(n_estimators=100)),\n",
    "])\n",
    "\n",
    "pipeline_rf.fit(X_train, y_train)\n",
    "y_pred_rf = pipeline_rf.predict(X_test)\n",
    "\n",
    "# Model Evaluation\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_rf))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01e613a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\12har\\anaconda3\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\12har\\anaconda3\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\12har\\anaconda3\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:From C:\\Users\\12har\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\12har\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "96/96 [==============================] - 68s 599ms/step - loss: 0.5524 - accuracy: 0.7148 - val_loss: 0.4629 - val_accuracy: 0.7899\n",
      "Epoch 2/10\n",
      "96/96 [==============================] - 55s 568ms/step - loss: 0.3125 - accuracy: 0.8726 - val_loss: 0.5158 - val_accuracy: 0.7728\n",
      "Epoch 3/10\n",
      "96/96 [==============================] - 60s 630ms/step - loss: 0.1914 - accuracy: 0.9333 - val_loss: 0.6141 - val_accuracy: 0.7695\n",
      "Epoch 4/10\n",
      "96/96 [==============================] - 60s 625ms/step - loss: 0.1287 - accuracy: 0.9578 - val_loss: 0.7581 - val_accuracy: 0.7689\n",
      "Epoch 5/10\n",
      "96/96 [==============================] - 60s 629ms/step - loss: 0.1049 - accuracy: 0.9640 - val_loss: 0.7602 - val_accuracy: 0.7761\n",
      "Epoch 6/10\n",
      "96/96 [==============================] - 59s 611ms/step - loss: 0.0943 - accuracy: 0.9703 - val_loss: 0.6694 - val_accuracy: 0.7505\n",
      "Epoch 7/10\n",
      "96/96 [==============================] - 59s 610ms/step - loss: 0.0811 - accuracy: 0.9757 - val_loss: 0.7816 - val_accuracy: 0.7485\n",
      "Epoch 8/10\n",
      "96/96 [==============================] - 60s 631ms/step - loss: 0.0675 - accuracy: 0.9783 - val_loss: 0.8346 - val_accuracy: 0.7597\n",
      "Epoch 9/10\n",
      "96/96 [==============================] - 59s 617ms/step - loss: 0.0590 - accuracy: 0.9793 - val_loss: 1.0179 - val_accuracy: 0.7492\n",
      "Epoch 10/10\n",
      "96/96 [==============================] - 54s 562ms/step - loss: 0.0534 - accuracy: 0.9795 - val_loss: 1.0109 - val_accuracy: 0.7518\n",
      "48/48 [==============================] - 9s 159ms/step\n",
      "Accuracy: 0.7518056467498359\n",
      "Confusion Matrix:\n",
      " [[711 163]\n",
      " [215 434]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.81      0.79       874\n",
      "           1       0.73      0.67      0.70       649\n",
      "\n",
      "    accuracy                           0.75      1523\n",
      "   macro avg       0.75      0.74      0.74      1523\n",
      "weighted avg       0.75      0.75      0.75      1523\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5. Deep Learning - LSTM (Long Short-Term Memory)\n",
    "\n",
    "# Tokenization and padding\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=100)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=100)\n",
    "\n",
    "# Build LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(tokenizer.word_index) + 1, 100))\n",
    "model.add(LSTM(256, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_pad, y_train, batch_size=64, epochs=10, validation_data=(X_test_pad, y_test))\n",
    "\n",
    "# Predictions\n",
    "y_pred_lstm = (model.predict(X_test_pad) > 0.5).astype(\"int32\")\n",
    "\n",
    "# Model Evaluation\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_lstm))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_lstm))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_lstm))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648a5fc5",
   "metadata": {},
   "source": [
    "#### Now that we've checked all the models accuracy, pick the highest model and train again without splitting train/test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "592679f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Logistic Regression model with TF-IDF for text vectorization\n",
    "\n",
    "X_train = df['text']\n",
    "y_train = df['target']\n",
    "X_test = test['text']\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(stop_words=stopwords.words('english'))),\n",
    "    ('clf', LogisticRegression(solver='liblinear')),\n",
    "])\n",
    "\n",
    "# Train the model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = pipeline.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6b7b868b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame(test['id'])\n",
    "result['target'] = y_pred\n",
    "result\n",
    "result.to_csv('predicted_targets.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d38dc2",
   "metadata": {},
   "source": [
    "Score: 0.78792"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f97538",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
